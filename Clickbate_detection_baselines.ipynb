{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c4de4b786d7e49ad88a13a4eea448092": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9811ca06d52a41e8ad9c2a0a88605801",
              "IPY_MODEL_2db01e900c7e4b919f738d0d4ee8add9",
              "IPY_MODEL_62d59e0e3a804ef38cd297701895c4a7"
            ],
            "layout": "IPY_MODEL_51b58323cfdc4c5f88c650c1d8454991"
          }
        },
        "9811ca06d52a41e8ad9c2a0a88605801": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa0e55a3c1964fadbd40e86105f6b278",
            "placeholder": "​",
            "style": "IPY_MODEL_8d24648ef1434e02a9bcc55c60766680",
            "value": "Downloading (…)solve/main/vocab.txt: 100%"
          }
        },
        "2db01e900c7e4b919f738d0d4ee8add9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60ca4d12f57f484da5027b35c7c56492",
            "max": 871891,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b9c2167bf03f4d31b31d610270d41488",
            "value": 871891
          }
        },
        "62d59e0e3a804ef38cd297701895c4a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0c132c1ef484fe6b9863e33fe6d18a8",
            "placeholder": "​",
            "style": "IPY_MODEL_ede88927390541ddbf2624f245c3bcb6",
            "value": " 872k/872k [00:00&lt;00:00, 5.57MB/s]"
          }
        },
        "51b58323cfdc4c5f88c650c1d8454991": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa0e55a3c1964fadbd40e86105f6b278": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d24648ef1434e02a9bcc55c60766680": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "60ca4d12f57f484da5027b35c7c56492": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9c2167bf03f4d31b31d610270d41488": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a0c132c1ef484fe6b9863e33fe6d18a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ede88927390541ddbf2624f245c3bcb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b8d0424ba33442b8712e8b7524062a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ee368deb6e5f49e483a62194751ea7ca",
              "IPY_MODEL_840a3654909945198b485cdf6d5d7a96",
              "IPY_MODEL_85373096fcb94d70b9bb51ff14f52faf"
            ],
            "layout": "IPY_MODEL_5fff29e2eccd4b85a0fde2b92dd24750"
          }
        },
        "ee368deb6e5f49e483a62194751ea7ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5931edbee084f5b8aebe9b372a96c35",
            "placeholder": "​",
            "style": "IPY_MODEL_93d569619ed54d9ea1adf156936e289d",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "840a3654909945198b485cdf6d5d7a96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed5be6aa263f4f9db0e9cd2164d85384",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4ac9a2e2c4a8416fb840790219b966a5",
            "value": 28
          }
        },
        "85373096fcb94d70b9bb51ff14f52faf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c69811b7d9d4c27b57a4fde47043491",
            "placeholder": "​",
            "style": "IPY_MODEL_b3b42bf9bf3b4f399e271028a1912eaa",
            "value": " 28.0/28.0 [00:00&lt;00:00, 781B/s]"
          }
        },
        "5fff29e2eccd4b85a0fde2b92dd24750": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5931edbee084f5b8aebe9b372a96c35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93d569619ed54d9ea1adf156936e289d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed5be6aa263f4f9db0e9cd2164d85384": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ac9a2e2c4a8416fb840790219b966a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2c69811b7d9d4c27b57a4fde47043491": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3b42bf9bf3b4f399e271028a1912eaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "617b422e287f40b6b193ee11cebe7630": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eafc60c811f647b6b70440064423b8d0",
              "IPY_MODEL_b2cf34e864f5440c958087b9d7364abb",
              "IPY_MODEL_b13f9c0ec79d44cfaef335cc1ea1ee78"
            ],
            "layout": "IPY_MODEL_d0079d6a60b94231b7859b9898ade44f"
          }
        },
        "eafc60c811f647b6b70440064423b8d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00860a41e68a49baa42f6467fdc0aa97",
            "placeholder": "​",
            "style": "IPY_MODEL_da08557abcc34301abf0e3a75c3589fc",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "b2cf34e864f5440c958087b9d7364abb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19badf5e69b442568e9b13d9c0e1870c",
            "max": 625,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_56325b95e8134c62b0b4c94cded6b345",
            "value": 625
          }
        },
        "b13f9c0ec79d44cfaef335cc1ea1ee78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41dfb777c2f5489d8441116d504a29b8",
            "placeholder": "​",
            "style": "IPY_MODEL_d4b3772e692645e3bf2b97d0119573c2",
            "value": " 625/625 [00:00&lt;00:00, 25.0kB/s]"
          }
        },
        "d0079d6a60b94231b7859b9898ade44f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00860a41e68a49baa42f6467fdc0aa97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da08557abcc34301abf0e3a75c3589fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "19badf5e69b442568e9b13d9c0e1870c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56325b95e8134c62b0b4c94cded6b345": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "41dfb777c2f5489d8441116d504a29b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4b3772e692645e3bf2b97d0119573c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BewMmdLH2M6z",
        "outputId": "500586e9-aa6b-4cf0-880a-a9da258270db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-23 16:31:10--  https://raw.githubusercontent.com/BunnyNoBugs/hse-headlines-project/main/data/test.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19026 (19K) [text/plain]\n",
            "Saving to: ‘test.csv.1’\n",
            "\n",
            "test.csv.1          100%[===================>]  18.58K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2023-03-23 16:31:10 (20.1 MB/s) - ‘test.csv.1’ saved [19026/19026]\n",
            "\n",
            "--2023-03-23 16:31:10--  https://raw.githubusercontent.com/BunnyNoBugs/hse-headlines-project/main/data/train.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 175396 (171K) [text/plain]\n",
            "Saving to: ‘train.csv.1’\n",
            "\n",
            "train.csv.1         100%[===================>] 171.29K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-03-23 16:31:10 (10.5 MB/s) - ‘train.csv.1’ saved [175396/175396]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/BunnyNoBugs/hse-headlines-project/main/data/test.csv\n",
        "!wget https://raw.githubusercontent.com/BunnyNoBugs/hse-headlines-project/main/data/train.csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/BunnyNoBugs/hse-headlines-project/main/data/CorpusOfNewsHeadlines_new-Rus_FINAL.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xM4aLpqSi4Ta",
        "outputId": "ccb8d297-c915-4b48-b961-f9a6efafa5c5"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-23 16:31:10--  https://raw.githubusercontent.com/BunnyNoBugs/hse-headlines-project/main/data/CorpusOfNewsHeadlines_new-Rus_FINAL.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 980205 (957K) [text/plain]\n",
            "Saving to: ‘CorpusOfNewsHeadlines_new-Rus_FINAL.csv.1’\n",
            "\n",
            "CorpusOfNewsHeadlin 100%[===================>] 957.23K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-03-23 16:31:11 (22.6 MB/s) - ‘CorpusOfNewsHeadlines_new-Rus_FINAL.csv.1’ saved [980205/980205]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF"
      ],
      "metadata": {
        "id": "ugc3Z7sh2vVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "gZCXmqzb2zp3"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')"
      ],
      "metadata": {
        "id": "htlQh95y3T0v"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "yq9RJOUs-SAb",
        "outputId": "2b74e6ab-ff2f-485a-e84c-3e1754a9240b"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               Headline       Type  \\\n",
              "0     В Питере семь лет простоял памятник не тому че...     Honest   \n",
              "1     США официально объявили о новых санкциях проти...     Honest   \n",
              "2     Facebook удалил страницу обсуждения штурма «Зо...     Honest   \n",
              "3                   Джонсон способен развалить Британию  Clickbait   \n",
              "4     Спортсмен–парапланерист из Москвы погиб в гора...     Honest   \n",
              "...                                                 ...        ...   \n",
              "1188  «Шашлык Live» собрал аншлаг. Почему о фестивал...     Honest   \n",
              "1189                        Роналду заговорил по-русски  Clickbait   \n",
              "1190     Двое мужчин в гаражах забили насмерть человека     Honest   \n",
              "1191  SSJ-100 СОВЕРШИЛ ЭКСТРЕННУЮ ПОСАДКУ В САМАРЕ И...     Honest   \n",
              "1192  Зеленский раскритиковал не пускавшего к нему л...     Honest   \n",
              "\n",
              "                                    Manipulative Device  \n",
              "0                                             Assertion  \n",
              "1                                              Assetion  \n",
              "2                                             Assertion  \n",
              "3     False actuality inference, Other inference(Sem...  \n",
              "4                                             Assertion  \n",
              "...                                                 ...  \n",
              "1188                                   Assertion, Quote  \n",
              "1189                            False quality inference  \n",
              "1190                                          Assertion  \n",
              "1191                                          Assertion  \n",
              "1192                         Pressupposition, Assertion  \n",
              "\n",
              "[1193 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f13b1583-6d8b-43ef-a5ba-08d7dc4546c4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Headline</th>\n",
              "      <th>Type</th>\n",
              "      <th>Manipulative Device</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>В Питере семь лет простоял памятник не тому че...</td>\n",
              "      <td>Honest</td>\n",
              "      <td>Assertion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>США официально объявили о новых санкциях проти...</td>\n",
              "      <td>Honest</td>\n",
              "      <td>Assetion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Facebook удалил страницу обсуждения штурма «Зо...</td>\n",
              "      <td>Honest</td>\n",
              "      <td>Assertion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Джонсон способен развалить Британию</td>\n",
              "      <td>Clickbait</td>\n",
              "      <td>False actuality inference, Other inference(Sem...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Спортсмен–парапланерист из Москвы погиб в гора...</td>\n",
              "      <td>Honest</td>\n",
              "      <td>Assertion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1188</th>\n",
              "      <td>«Шашлык Live» собрал аншлаг. Почему о фестивал...</td>\n",
              "      <td>Honest</td>\n",
              "      <td>Assertion, Quote</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1189</th>\n",
              "      <td>Роналду заговорил по-русски</td>\n",
              "      <td>Clickbait</td>\n",
              "      <td>False quality inference</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1190</th>\n",
              "      <td>Двое мужчин в гаражах забили насмерть человека</td>\n",
              "      <td>Honest</td>\n",
              "      <td>Assertion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1191</th>\n",
              "      <td>SSJ-100 СОВЕРШИЛ ЭКСТРЕННУЮ ПОСАДКУ В САМАРЕ И...</td>\n",
              "      <td>Honest</td>\n",
              "      <td>Assertion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1192</th>\n",
              "      <td>Зеленский раскритиковал не пускавшего к нему л...</td>\n",
              "      <td>Honest</td>\n",
              "      <td>Pressupposition, Assertion</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1193 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f13b1583-6d8b-43ef-a5ba-08d7dc4546c4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f13b1583-6d8b-43ef-a5ba-08d7dc4546c4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f13b1583-6d8b-43ef-a5ba-08d7dc4546c4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = train[train['Type'] != 'Dishonest']\n",
        "test = test[test['Type'] != 'Dishonest']"
      ],
      "metadata": {
        "id": "hI6zB9X66idd"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_mapper = {'Honest': 0, 'Clickbait': 1}"
      ],
      "metadata": {
        "id": "X4c7RgEJ2oW8"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vec = TfidfVectorizer()\n",
        "vec.fit(train['Headline'].values)\n",
        "vec.fit(test['Headline'].values)\n",
        "X_train = vec.transform(train['Headline'].values)\n",
        "X_test = vec.transform(test['Headline'].values)\n",
        "y_train = train['Type'].map(cat_mapper)\n",
        "y_test = test['Type'].map(cat_mapper)"
      ],
      "metadata": {
        "id": "N9rdANG_3n9H"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hMyjCuS91WE",
        "outputId": "1aee74bf-03a9-49d8-fff5-6751301ac78a"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1165, 811), (128, 811), (1165,), (128,))"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "log_reg = LogisticRegression(class_weight='balanced')\n",
        "log_reg.fit(X_train, y_train)\n",
        "preds = log_reg.predict(X_test)\n",
        "print(f1_score(preds, y_test))\n",
        "print(accuracy_score(preds, y_test))\n",
        "print(precision_score(preds, y_test))\n",
        "print(recall_score(preds, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icHPkyLB6Qow",
        "outputId": "fb6d7511-b28a-49fc-96be-6455a4cc839b"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.21212121212121213\n",
            "0.59375\n",
            "0.25925925925925924\n",
            "0.1794871794871795\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bert"
      ],
      "metadata": {
        "id": "_NdFVk3zcWUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4J-1UJJYcZTd",
        "outputId": "896b16c6-04bc-4ba2-dd08-28e25ec21ae1"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():    \n",
        "    device = torch.device(\"cuda\")"
      ],
      "metadata": {
        "id": "xpV5BghgcoEL"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')"
      ],
      "metadata": {
        "id": "kA0Pb8hFc1vn"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "0 - Honest, 1 - Dishonest"
      ],
      "metadata": {
        "id": "HDv_mhYcx3f4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_train = train.Headline.values\n",
        "labels_train = train.Type.map(cat_mapper).values\n",
        "sentences_test = train.Headline.values\n",
        "labels_test = train.Type.map(cat_mapper).values"
      ],
      "metadata": {
        "id": "yV6ibkBpea-A"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids_train = []\n",
        "input_ids_test = []\n",
        "\n",
        "for sent in sentences_train:\n",
        "    encoded_sent = tokenizer.encode(sent, add_special_tokens = True)\n",
        "    input_ids_train.append(encoded_sent)\n",
        "\n",
        "for sent in sentences_test:\n",
        "    encoded_sent = tokenizer.encode(sent, add_special_tokens = True)\n",
        "    input_ids_test.append(encoded_sent)"
      ],
      "metadata": {
        "id": "PabeDmebeHXe"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras_preprocessing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yfpOphr2vcD",
        "outputId": "d0e9a4aa-170d-4852-846d-152949e3cbc6"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras_preprocessing in /usr/local/lib/python3.9/dist-packages (1.1.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from keras_preprocessing) (1.16.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.9/dist-packages (from keras_preprocessing) (1.22.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras_preprocessing.sequence import pad_sequences\n",
        "\n",
        "MAX_LEN = 80\n",
        "\n",
        "input_ids_train = pad_sequences(input_ids_train, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "input_ids_test = pad_sequences(input_ids_test, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")"
      ],
      "metadata": {
        "id": "cCtXaEDdfCQ4"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention_masks_train = []\n",
        "attention_masks_test = []\n",
        "\n",
        "for sent in input_ids_train:\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    attention_masks_train.append(att_mask)\n",
        "\n",
        "for sent in input_ids_test:\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    attention_masks_test.append(att_mask)"
      ],
      "metadata": {
        "id": "HUBWkinsfKsB"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_inputs = torch.tensor(input_ids_train)\n",
        "validation_inputs = torch.tensor(input_ids_test)\n",
        "\n",
        "train_labels = torch.tensor(labels_train)\n",
        "validation_labels = torch.tensor(labels_test)\n",
        "\n",
        "train_masks = torch.tensor(attention_masks_train)\n",
        "validation_masks = torch.tensor(attention_masks_test)"
      ],
      "metadata": {
        "id": "ZPpdGmyjfOf-"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "5AzUJ72pxnGo"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AdamW, BertConfig, BertForSequenceClassification"
      ],
      "metadata": {
        "id": "e5o2TS6V0Uwz"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-multilingual-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwSeehI71TAt",
        "outputId": "9dfd823d-852c-4893-a3d0-2047ed5577ee"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(105879, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOGVAfQ-0kJE",
        "outputId": "9ac83720-c257-46c0-f748-58901c04b58b"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 100, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "metadata": {
        "id": "JmvSPJz008Dq"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return f1_score(pred_flat, labels_flat)\n",
        "    #return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "metadata": {
        "id": "-4Cm0Dre1jBA"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "metadata": {
        "id": "dMcvT3ds1nef"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "loss_values = []\n",
        "\n",
        "for epoch_i in range(0, 8):\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    t0 = time.time()\n",
        "    total_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        #b_labels = torch.nn.functional.one_hot(b_labels)\n",
        "\n",
        "        model.zero_grad()        \n",
        "\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "    eval_f1, eval_precision, eval_recall = 0, 0, 0\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_f1 = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_f1 += tmp_eval_f1\n",
        "        #eval_precision += tmp_eval_precision\n",
        "        #eval_recall += tmp_eval_recall\n",
        "        #eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    #print(\"  accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  f1: {0:.2f}\".format(eval_f1/nb_eval_steps))\n",
        "    #print(\"  precision: {0:.2f}\".format(eval_precision/nb_eval_steps))\n",
        "    #print(\"  recall: {0:.2f}\".format(eval_recall/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enLlns9517Oq",
        "outputId": "2a04f1e9-464b-4ed9-d4ed-cee2ab67d210"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.64\n",
            "  Training epcoh took: 0:00:16\n",
            "\n",
            "Running Validation...\n",
            "  f1: 0.00\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.54\n",
            "  Training epcoh took: 0:00:16\n",
            "\n",
            "Running Validation...\n",
            "  f1: 0.00\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.49\n",
            "  Training epcoh took: 0:00:17\n",
            "\n",
            "Running Validation...\n",
            "  f1: 0.23\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.41\n",
            "  Training epcoh took: 0:00:16\n",
            "\n",
            "Running Validation...\n",
            "  f1: 0.75\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "======== Epoch 5 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epcoh took: 0:00:16\n",
            "\n",
            "Running Validation...\n",
            "  f1: 0.75\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "======== Epoch 6 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epcoh took: 0:00:16\n",
            "\n",
            "Running Validation...\n",
            "  f1: 0.75\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "======== Epoch 7 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epcoh took: 0:00:16\n",
            "\n",
            "Running Validation...\n",
            "  f1: 0.75\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "======== Epoch 8 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epcoh took: 0:00:16\n",
            "\n",
            "Running Validation...\n",
            "  f1: 0.75\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# С текстами статей"
      ],
      "metadata": {
        "id": "OxQRQf0irLEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('output_text_final.csv', sep = ';')"
      ],
      "metadata": {
        "id": "M_rlfJArrOBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "OaLSNGANkj1F",
        "outputId": "cd84b022-647d-48ae-e3da-364371d75a5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0                                           Headline        Date  \\\n",
              "0           0  Разоблачитель российского допинга взялся за бр...  30.06.2018   \n",
              "1           1  Главный тренер сборной Испании признал опаснос...  30.06.2018   \n",
              "2           2  Дзюба признался в готовности встать на ворота ...  30.06.2018   \n",
              "3           3  Макаревич пригрозил силовикам адом за преследо...  30.06.2018   \n",
              "4           4           Власти объяснили введение платы за дождь  30.06.2018   \n",
              "\n",
              "     Source                                       Link        Broad Topic  \\\n",
              "0  lenta.ru  https://lenta.ru/news/2018/06/30/zeppelt/             Sports   \n",
              "1  lenta.ru  https://lenta.ru/news/2018/06/30/dangers/             Sports   \n",
              "2  lenta.ru   https://lenta.ru/news/2018/06/30/dzyuba/             Sports   \n",
              "3  lenta.ru     https://lenta.ru/news/2018/06/30/hell/    Politics, Stars   \n",
              "4  lenta.ru     https://lenta.ru/news/2018/06/30/rain/  Urban development   \n",
              "\n",
              "        Type                                Manipulative Device  \\\n",
              "0     Honest                                          Assertion   \n",
              "1     Honest                                          Assertion   \n",
              "2  Clickbait                          False actuality inference   \n",
              "3  Clickbait  False referentiality inference, Other inferenc...   \n",
              "4  Clickbait  False actuality inference, False referentialit...   \n",
              "\n",
              "                                Html-marked Headline  \\\n",
              "0  Разоблачитель российского допинга взялся за бр...   \n",
              "1  Главный тренер сборной Испании <trigger><ev:po...   \n",
              "2  Дзюба <trigger><ev:neg>признался</ev:neg><trig...   \n",
              "3  Макаревич <homonymy:sem>пригрозил силовикам <e...   \n",
              "4  Власти <trigger>объяснили</trigger> <trigger>в...   \n",
              "\n",
              "                                             Summary  \\\n",
              "0  Хайо Зеппельт, расследоваший применения допинг...   \n",
              "1        Тренер считает Россию серьёзным соперником.   \n",
              "2  Дзюба сказал, что встанет на ворота только в с...   \n",
              "3  Музыкант сказал: \"Ребят, ну не позорьтесь вы. ...   \n",
              "4  Власти Кунгура объяснили, что налога за дождь ...   \n",
              "\n",
              "                                                Text  \n",
              "0  Немецкий журналист Хайо Зеппельт анонсировал д...  \n",
              "1  Главный тренер сборной Испании по футболу Ферн...  \n",
              "2  Нападающий сборной России Артем Дзюба не исклю...  \n",
              "3  Лидер группы «Машина времени» Андрей Макаревич...  \n",
              "4  Власти города Кунгур ответили на вопрос о введ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1389cc1f-31a5-414b-8a70-6ed02cb7a22f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Headline</th>\n",
              "      <th>Date</th>\n",
              "      <th>Source</th>\n",
              "      <th>Link</th>\n",
              "      <th>Broad Topic</th>\n",
              "      <th>Type</th>\n",
              "      <th>Manipulative Device</th>\n",
              "      <th>Html-marked Headline</th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Разоблачитель российского допинга взялся за бр...</td>\n",
              "      <td>30.06.2018</td>\n",
              "      <td>lenta.ru</td>\n",
              "      <td>https://lenta.ru/news/2018/06/30/zeppelt/</td>\n",
              "      <td>Sports</td>\n",
              "      <td>Honest</td>\n",
              "      <td>Assertion</td>\n",
              "      <td>Разоблачитель российского допинга взялся за бр...</td>\n",
              "      <td>Хайо Зеппельт, расследоваший применения допинг...</td>\n",
              "      <td>Немецкий журналист Хайо Зеппельт анонсировал д...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Главный тренер сборной Испании признал опаснос...</td>\n",
              "      <td>30.06.2018</td>\n",
              "      <td>lenta.ru</td>\n",
              "      <td>https://lenta.ru/news/2018/06/30/dangers/</td>\n",
              "      <td>Sports</td>\n",
              "      <td>Honest</td>\n",
              "      <td>Assertion</td>\n",
              "      <td>Главный тренер сборной Испании &lt;trigger&gt;&lt;ev:po...</td>\n",
              "      <td>Тренер считает Россию серьёзным соперником.</td>\n",
              "      <td>Главный тренер сборной Испании по футболу Ферн...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Дзюба признался в готовности встать на ворота ...</td>\n",
              "      <td>30.06.2018</td>\n",
              "      <td>lenta.ru</td>\n",
              "      <td>https://lenta.ru/news/2018/06/30/dzyuba/</td>\n",
              "      <td>Sports</td>\n",
              "      <td>Clickbait</td>\n",
              "      <td>False actuality inference</td>\n",
              "      <td>Дзюба &lt;trigger&gt;&lt;ev:neg&gt;признался&lt;/ev:neg&gt;&lt;trig...</td>\n",
              "      <td>Дзюба сказал, что встанет на ворота только в с...</td>\n",
              "      <td>Нападающий сборной России Артем Дзюба не исклю...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Макаревич пригрозил силовикам адом за преследо...</td>\n",
              "      <td>30.06.2018</td>\n",
              "      <td>lenta.ru</td>\n",
              "      <td>https://lenta.ru/news/2018/06/30/hell/</td>\n",
              "      <td>Politics, Stars</td>\n",
              "      <td>Clickbait</td>\n",
              "      <td>False referentiality inference, Other inferenc...</td>\n",
              "      <td>Макаревич &lt;homonymy:sem&gt;пригрозил силовикам &lt;e...</td>\n",
              "      <td>Музыкант сказал: \"Ребят, ну не позорьтесь вы. ...</td>\n",
              "      <td>Лидер группы «Машина времени» Андрей Макаревич...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Власти объяснили введение платы за дождь</td>\n",
              "      <td>30.06.2018</td>\n",
              "      <td>lenta.ru</td>\n",
              "      <td>https://lenta.ru/news/2018/06/30/rain/</td>\n",
              "      <td>Urban development</td>\n",
              "      <td>Clickbait</td>\n",
              "      <td>False actuality inference, False referentialit...</td>\n",
              "      <td>Власти &lt;trigger&gt;объяснили&lt;/trigger&gt; &lt;trigger&gt;в...</td>\n",
              "      <td>Власти Кунгура объяснили, что налога за дождь ...</td>\n",
              "      <td>Власти города Кунгур ответили на вопрос о введ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1389cc1f-31a5-414b-8a70-6ed02cb7a22f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1389cc1f-31a5-414b-8a70-6ed02cb7a22f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1389cc1f-31a5-414b-8a70-6ed02cb7a22f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[df['Type'] == 'Clickbait']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpK42_c1kbx-",
        "outputId": "3a4524d0-1bad-49d1-ea6e-a77526cb1982"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Unnamed: 0                                           Headline  \\\n",
              "2              2  Дзюба признался в готовности встать на ворота ...   \n",
              "3              3  Макаревич пригрозил силовикам адом за преследо...   \n",
              "4              4           Власти объяснили введение платы за дождь   \n",
              "5              5  «Собрался весь двор и надавали тумаков» Парень...   \n",
              "6              6  Штыки друзей. Почему Приднестровье не хочет от...   \n",
              "...          ...                                                ...   \n",
              "1078        1078  Скончался индийский йог, который не пил и не е...   \n",
              "1080        1080  Джо Байден растерял рейтинг одобрения после за...   \n",
              "1082        1082  Работавшего на Пентагон афганца могут выслать ...   \n",
              "1083        1083  Депутат-миллионер чистосердечно признался, что...   \n",
              "1084        1084  Олимпийской чемпионке поменяют золотую медаль ...   \n",
              "\n",
              "            Date       Source  \\\n",
              "2     30.06.2018     lenta.ru   \n",
              "3     30.06.2018     lenta.ru   \n",
              "4     30.06.2018     lenta.ru   \n",
              "5     30.06.2018     lenta.ru   \n",
              "6     30.06.2018     lenta.ru   \n",
              "...          ...          ...   \n",
              "1078  27.05.2021  Ридуc/Ridus   \n",
              "1080  18.08.2021  Ридуc/Ridus   \n",
              "1082  07.07.2021  Ридуc/Ridus   \n",
              "1083  10.08.2021  Ридуc/Ridus   \n",
              "1084  12.08.2021  Ридуc/Ridus   \n",
              "\n",
              "                                                   Link         Broad Topic  \\\n",
              "2              https://lenta.ru/news/2018/06/30/dzyuba/              Sports   \n",
              "3                https://lenta.ru/news/2018/06/30/hell/     Politics, Stars   \n",
              "4                https://lenta.ru/news/2018/06/30/rain/   Urban development   \n",
              "5            https://lenta.ru/articles/2018/06/30/stas/           Incidents   \n",
              "6     https://lenta.ru/articles/2018/06/30/transnist...       Politics, War   \n",
              "...                                                 ...                 ...   \n",
              "1078                   https://www.ridus.ru/news/328227             Society   \n",
              "1080                   https://www.ridus.ru/news/360459            Politics   \n",
              "1082                   https://www.ridus.ru/news/357667        War, Society   \n",
              "1083                   https://www.ridus.ru/news/359944           Incidents   \n",
              "1084                   https://www.ridus.ru/news/360128  Incidents, Society   \n",
              "\n",
              "           Type                                Manipulative Device  \\\n",
              "2     Clickbait                          False actuality inference   \n",
              "3     Clickbait  False referentiality inference, Other inferenc...   \n",
              "4     Clickbait  False actuality inference, False referentialit...   \n",
              "5     Clickbait  Other inference(Hyperbole), Other inference(Se...   \n",
              "6     Clickbait      Other inference(Not the topic of the article)   \n",
              "...         ...                                                ...   \n",
              "1078  Clickbait                            False quality inference   \n",
              "1080  Clickbait                         Other inference(Hyperbole)   \n",
              "1082  Clickbait  False referentiality inference, False cause-ef...   \n",
              "1083  Clickbait  False cause-effect inference, False factuality...   \n",
              "1084  Clickbait                     False referentiality inference   \n",
              "\n",
              "                                   Html-marked Headline  \\\n",
              "2     Дзюба <trigger><ev:neg>признался</ev:neg><trig...   \n",
              "3     Макаревич <homonymy:sem>пригрозил силовикам <e...   \n",
              "4     Власти <trigger>объяснили</trigger> <trigger>в...   \n",
              "5     «Собрался весь двор и надавали тумаков» Парень...   \n",
              "6     Штыки друзей. <trigger><question>Почему</quest...   \n",
              "...                                                 ...   \n",
              "1078  Скончался индийский йог, <trigger>который</tri...   \n",
              "1080  Джо Байден <hyperbole>растерял</hyperbole> рей...   \n",
              "1082  Работавшего на Пентагон афганца могут выслать ...   \n",
              "1083  Депутат-миллионер чистосердечно признался, что...   \n",
              "1084  Олимпийской чемпионке поменяют золотую медаль ...   \n",
              "\n",
              "                                                Summary  \\\n",
              "2     Дзюба сказал, что встанет на ворота только в с...   \n",
              "3     Музыкант сказал: \"Ребят, ну не позорьтесь вы. ...   \n",
              "4     Власти Кунгура объяснили, что налога за дождь ...   \n",
              "5     Юноша с ДЦП толкнул и ударил мальчика, который...   \n",
              "6     Правительство и жители Приднестровья выступают...   \n",
              "...                                                 ...   \n",
              "1078  Умер йог, по словам которого он не ел с 12 до ...   \n",
              "1080  После вывода войск из Афганистана рейтинг Байд...   \n",
              "1082  Афганца, жителя США, могут потенциально выслат...   \n",
              "1083  Депутат охотился на медведя, сделал выстрел дл...   \n",
              "1084  Довольный победой мэр попробовал золотую медал...   \n",
              "\n",
              "                                                   Text  \n",
              "2     Нападающий сборной России Артем Дзюба не исклю...  \n",
              "3     Лидер группы «Машина времени» Андрей Макаревич...  \n",
              "4     Власти города Кунгур ответили на вопрос о введ...  \n",
              "5     Тысячи пользователей соцсетей поддержали 21-ле...  \n",
              "6     В конце июня российских военных «попросили на ...  \n",
              "...                                                 ...  \n",
              "1078  Йог и отшельник Прахлад Джани, утверждавший, ч...  \n",
              "1080  Сдача Афганистана боевикам исламистского движе...  \n",
              "1082  Гражданин Афганистана Залмай Ниязи, состоявший...  \n",
              "1083  Депутат Законодательного собрания Камчатского ...  \n",
              "1084  Японской софтболистке Миу Гото, завоевавшей зо...  \n",
              "\n",
              "[262 rows x 11 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f8a410ab-a50e-4bf0-b406-dc5ad4fc3c46\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Headline</th>\n",
              "      <th>Date</th>\n",
              "      <th>Source</th>\n",
              "      <th>Link</th>\n",
              "      <th>Broad Topic</th>\n",
              "      <th>Type</th>\n",
              "      <th>Manipulative Device</th>\n",
              "      <th>Html-marked Headline</th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Дзюба признался в готовности встать на ворота ...</td>\n",
              "      <td>30.06.2018</td>\n",
              "      <td>lenta.ru</td>\n",
              "      <td>https://lenta.ru/news/2018/06/30/dzyuba/</td>\n",
              "      <td>Sports</td>\n",
              "      <td>Clickbait</td>\n",
              "      <td>False actuality inference</td>\n",
              "      <td>Дзюба &lt;trigger&gt;&lt;ev:neg&gt;признался&lt;/ev:neg&gt;&lt;trig...</td>\n",
              "      <td>Дзюба сказал, что встанет на ворота только в с...</td>\n",
              "      <td>Нападающий сборной России Артем Дзюба не исклю...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Макаревич пригрозил силовикам адом за преследо...</td>\n",
              "      <td>30.06.2018</td>\n",
              "      <td>lenta.ru</td>\n",
              "      <td>https://lenta.ru/news/2018/06/30/hell/</td>\n",
              "      <td>Politics, Stars</td>\n",
              "      <td>Clickbait</td>\n",
              "      <td>False referentiality inference, Other inferenc...</td>\n",
              "      <td>Макаревич &lt;homonymy:sem&gt;пригрозил силовикам &lt;e...</td>\n",
              "      <td>Музыкант сказал: \"Ребят, ну не позорьтесь вы. ...</td>\n",
              "      <td>Лидер группы «Машина времени» Андрей Макаревич...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Власти объяснили введение платы за дождь</td>\n",
              "      <td>30.06.2018</td>\n",
              "      <td>lenta.ru</td>\n",
              "      <td>https://lenta.ru/news/2018/06/30/rain/</td>\n",
              "      <td>Urban development</td>\n",
              "      <td>Clickbait</td>\n",
              "      <td>False actuality inference, False referentialit...</td>\n",
              "      <td>Власти &lt;trigger&gt;объяснили&lt;/trigger&gt; &lt;trigger&gt;в...</td>\n",
              "      <td>Власти Кунгура объяснили, что налога за дождь ...</td>\n",
              "      <td>Власти города Кунгур ответили на вопрос о введ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>«Собрался весь двор и надавали тумаков» Парень...</td>\n",
              "      <td>30.06.2018</td>\n",
              "      <td>lenta.ru</td>\n",
              "      <td>https://lenta.ru/articles/2018/06/30/stas/</td>\n",
              "      <td>Incidents</td>\n",
              "      <td>Clickbait</td>\n",
              "      <td>Other inference(Hyperbole), Other inference(Se...</td>\n",
              "      <td>«Собрался весь двор и надавали тумаков» Парень...</td>\n",
              "      <td>Юноша с ДЦП толкнул и ударил мальчика, который...</td>\n",
              "      <td>Тысячи пользователей соцсетей поддержали 21-ле...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>Штыки друзей. Почему Приднестровье не хочет от...</td>\n",
              "      <td>30.06.2018</td>\n",
              "      <td>lenta.ru</td>\n",
              "      <td>https://lenta.ru/articles/2018/06/30/transnist...</td>\n",
              "      <td>Politics, War</td>\n",
              "      <td>Clickbait</td>\n",
              "      <td>Other inference(Not the topic of the article)</td>\n",
              "      <td>Штыки друзей. &lt;trigger&gt;&lt;question&gt;Почему&lt;/quest...</td>\n",
              "      <td>Правительство и жители Приднестровья выступают...</td>\n",
              "      <td>В конце июня российских военных «попросили на ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1078</th>\n",
              "      <td>1078</td>\n",
              "      <td>Скончался индийский йог, который не пил и не е...</td>\n",
              "      <td>27.05.2021</td>\n",
              "      <td>Ридуc/Ridus</td>\n",
              "      <td>https://www.ridus.ru/news/328227</td>\n",
              "      <td>Society</td>\n",
              "      <td>Clickbait</td>\n",
              "      <td>False quality inference</td>\n",
              "      <td>Скончался индийский йог, &lt;trigger&gt;который&lt;/tri...</td>\n",
              "      <td>Умер йог, по словам которого он не ел с 12 до ...</td>\n",
              "      <td>Йог и отшельник Прахлад Джани, утверждавший, ч...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1080</th>\n",
              "      <td>1080</td>\n",
              "      <td>Джо Байден растерял рейтинг одобрения после за...</td>\n",
              "      <td>18.08.2021</td>\n",
              "      <td>Ридуc/Ridus</td>\n",
              "      <td>https://www.ridus.ru/news/360459</td>\n",
              "      <td>Politics</td>\n",
              "      <td>Clickbait</td>\n",
              "      <td>Other inference(Hyperbole)</td>\n",
              "      <td>Джо Байден &lt;hyperbole&gt;растерял&lt;/hyperbole&gt; рей...</td>\n",
              "      <td>После вывода войск из Афганистана рейтинг Байд...</td>\n",
              "      <td>Сдача Афганистана боевикам исламистского движе...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1082</th>\n",
              "      <td>1082</td>\n",
              "      <td>Работавшего на Пентагон афганца могут выслать ...</td>\n",
              "      <td>07.07.2021</td>\n",
              "      <td>Ридуc/Ridus</td>\n",
              "      <td>https://www.ridus.ru/news/357667</td>\n",
              "      <td>War, Society</td>\n",
              "      <td>Clickbait</td>\n",
              "      <td>False referentiality inference, False cause-ef...</td>\n",
              "      <td>Работавшего на Пентагон афганца могут выслать ...</td>\n",
              "      <td>Афганца, жителя США, могут потенциально выслат...</td>\n",
              "      <td>Гражданин Афганистана Залмай Ниязи, состоявший...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1083</th>\n",
              "      <td>1083</td>\n",
              "      <td>Депутат-миллионер чистосердечно признался, что...</td>\n",
              "      <td>10.08.2021</td>\n",
              "      <td>Ридуc/Ridus</td>\n",
              "      <td>https://www.ridus.ru/news/359944</td>\n",
              "      <td>Incidents</td>\n",
              "      <td>Clickbait</td>\n",
              "      <td>False cause-effect inference, False factuality...</td>\n",
              "      <td>Депутат-миллионер чистосердечно признался, что...</td>\n",
              "      <td>Депутат охотился на медведя, сделал выстрел дл...</td>\n",
              "      <td>Депутат Законодательного собрания Камчатского ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1084</th>\n",
              "      <td>1084</td>\n",
              "      <td>Олимпийской чемпионке поменяют золотую медаль ...</td>\n",
              "      <td>12.08.2021</td>\n",
              "      <td>Ридуc/Ridus</td>\n",
              "      <td>https://www.ridus.ru/news/360128</td>\n",
              "      <td>Incidents, Society</td>\n",
              "      <td>Clickbait</td>\n",
              "      <td>False referentiality inference</td>\n",
              "      <td>Олимпийской чемпионке поменяют золотую медаль ...</td>\n",
              "      <td>Довольный победой мэр попробовал золотую медал...</td>\n",
              "      <td>Японской софтболистке Миу Гото, завоевавшей зо...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>262 rows × 11 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f8a410ab-a50e-4bf0-b406-dc5ad4fc3c46')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f8a410ab-a50e-4bf0-b406-dc5ad4fc3c46 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f8a410ab-a50e-4bf0-b406-dc5ad4fc3c46');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[df['Type'] != 'Dishonest']"
      ],
      "metadata": {
        "id": "OPTgasW5_-XA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_mapper = {'Honest': 0, 'Clickbait': 1}"
      ],
      "metadata": {
        "id": "pQ7t7RTxEeAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[pd.isna(df['Text']) == False]\n",
        "headlines = df['Headline'].values\n",
        "texts = df['Text'].values\n",
        "labels = df['Type'].map(cat_mapper).values"
      ],
      "metadata": {
        "id": "kNVtmv5lvcyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():    \n",
        "    device = torch.device(\"cuda\")"
      ],
      "metadata": {
        "id": "ONaeTo8hEiTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A57mARoObjQc",
        "outputId": "545c94eb-6e38-4a57-a9bd-e0ed55380082"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.27.2-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.3 tokenizers-0.13.2 transformers-4.27.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')"
      ],
      "metadata": {
        "id": "vZ3SunKXEbmb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "c4de4b786d7e49ad88a13a4eea448092",
            "9811ca06d52a41e8ad9c2a0a88605801",
            "2db01e900c7e4b919f738d0d4ee8add9",
            "62d59e0e3a804ef38cd297701895c4a7",
            "51b58323cfdc4c5f88c650c1d8454991",
            "fa0e55a3c1964fadbd40e86105f6b278",
            "8d24648ef1434e02a9bcc55c60766680",
            "60ca4d12f57f484da5027b35c7c56492",
            "b9c2167bf03f4d31b31d610270d41488",
            "a0c132c1ef484fe6b9863e33fe6d18a8",
            "ede88927390541ddbf2624f245c3bcb6",
            "0b8d0424ba33442b8712e8b7524062a7",
            "ee368deb6e5f49e483a62194751ea7ca",
            "840a3654909945198b485cdf6d5d7a96",
            "85373096fcb94d70b9bb51ff14f52faf",
            "5fff29e2eccd4b85a0fde2b92dd24750",
            "c5931edbee084f5b8aebe9b372a96c35",
            "93d569619ed54d9ea1adf156936e289d",
            "ed5be6aa263f4f9db0e9cd2164d85384",
            "4ac9a2e2c4a8416fb840790219b966a5",
            "2c69811b7d9d4c27b57a4fde47043491",
            "b3b42bf9bf3b4f399e271028a1912eaa",
            "617b422e287f40b6b193ee11cebe7630",
            "eafc60c811f647b6b70440064423b8d0",
            "b2cf34e864f5440c958087b9d7364abb",
            "b13f9c0ec79d44cfaef335cc1ea1ee78",
            "d0079d6a60b94231b7859b9898ade44f",
            "00860a41e68a49baa42f6467fdc0aa97",
            "da08557abcc34301abf0e3a75c3589fc",
            "19badf5e69b442568e9b13d9c0e1870c",
            "56325b95e8134c62b0b4c94cded6b345",
            "41dfb777c2f5489d8441116d504a29b8",
            "d4b3772e692645e3bf2b97d0119573c2"
          ]
        },
        "outputId": "efe0a981-d5b8-4a01-a7d6-47da19b6106f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/872k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c4de4b786d7e49ad88a13a4eea448092"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0b8d0424ba33442b8712e8b7524062a7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "617b422e287f40b6b193ee11cebe7630"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = []\n",
        "for ind, headline in enumerate(headlines):\n",
        "    encoded_sent = tokenizer(headline, texts[ind], add_special_tokens = True, max_length = 512, truncation = True, padding = 'max_length')\n",
        "    input_ids.append(encoded_sent)"
      ],
      "metadata": {
        "id": "OWglHbPVuUco",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcb46a9b-c20c-4de7-8d1b-b5104b60354d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras_preprocessing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiYot_LMuGLS",
        "outputId": "bd1b0926-f46b-435e-e095-0ce02ebfe727"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras_preprocessing in /usr/local/lib/python3.9/dist-packages (1.1.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from keras_preprocessing) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.9/dist-packages (from keras_preprocessing) (1.22.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras_preprocessing.sequence import pad_sequences\n",
        "\n",
        "MAX_LEN = 512\n",
        "\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")"
      ],
      "metadata": {
        "id": "wKbYoQkTuqID",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "aba6f751-d43f-4faa-93c4-0a75ba824fe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-22403c319618>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mMAX_LEN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n\u001b[0m\u001b[1;32m      6\u001b[0m                           value=0, truncating=\"post\", padding=\"post\")\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras_preprocessing/sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[0;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mtrunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtruncating\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'post'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0mtrunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             raise ValueError('Truncating type \"%s\" '\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encodings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m             raise KeyError(\n\u001b[0m\u001b[1;32m    242\u001b[0m                 \u001b[0;34m\"Indexing with integers (to access backend Encoding for a given batch index) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0;34m\"is not available when using Python based tokenizers\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Indexing with integers (to access backend Encoding for a given batch index) is not available when using Python based tokenizers'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention_masks = []\n",
        "\n",
        "for sent in input_ids:\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    attention_masks.append(att_mask)"
      ],
      "metadata": {
        "id": "OIfoqTlcybhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "T8Bz_zzurmnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_data = []\n",
        "for ind, text in enumerate(input_ids):\n",
        "    text['targets'] = labels[ind]\n",
        "    input_data.append(text)"
      ],
      "metadata": {
        "id": "tpxPoaUpRtBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_inputs, validation_inputs = train_test_split(input_ids, test_size = 0.3)\n",
        "#train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels, test_size=0.3)"
      ],
      "metadata": {
        "id": "3ZKRKq_DsEXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "\n",
        "#train_masks = torch.tensor(train_masks)\n",
        "#validation_masks = torch.tensor(validation_masks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "AfnI3Aub0roH",
        "outputId": "186440ed-be1f-4b22-e199-bcd74542df36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-642678ed0e56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mvalidation_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not determine the shape of object type 'BatchEncoding'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "#train_data = TensorDataset(train_inputs, train_labels)\n",
        "#train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_inputs, batch_size=batch_size)\n",
        "\n",
        "#validation_data = TensorDataset(validation_inputs, validation_labels)\n",
        "#validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_inputs, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "i4-aFVhQsbhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in train_dataloader:\n",
        "    print(batch)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwmXG7k8Tl3t",
        "outputId": "a1bfb86d-8492-4819-8d6a-2c7a2409a56d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [tensor([101, 101, 101, 101]), tensor([10291,   309, 39800, 47706]), tensor([92025,   330, 47233, 11637]), tensor([91202, 56138, 22503, 13646]), tensor([10185, 49460, 80863, 53464]), tensor([90137, 30965, 43039, 10392]), tensor([34797, 10353, 11863, 17940]), tensor([14597, 63541,   309,   118]), tensor([47171, 10178, 33354, 54195]), tensor([10353, 10396, 18745, 10360]), tensor([10842,   321, 45824, 93857]), tensor([16696, 51182, 12690, 32690]), tensor([54994, 11569, 11182, 10360]), tensor([75670,   309,   323, 46556]), tensor([72621, 59920, 19646,   309]), tensor([11843, 10328, 44052, 92315]), tensor([15428, 17659, 10475, 25639]), tensor([11141,   309,   102, 57247]), tensor([  309,   107, 39800, 65862]), tensor([24853,   314, 47233, 10260]), tensor([12415, 18076, 22503,   309]), tensor([  102, 45905, 80863, 44662]), tensor([33878, 10468, 43039, 11487]), tensor([64857, 74517, 11863, 90064]), tensor([  117, 12253,   309,   102]), tensor([10887,   107, 33354, 10242]), tensor([10396,   102, 18745, 11841]), tensor([45267, 42460, 45824, 19458]), tensor([10386, 10432, 12690, 18282]), tensor([16646, 11637, 11182, 25533]), tensor([32974, 14664,   323, 11311]), tensor([10726, 85515, 19646, 67877]), tensor([  309, 16991, 44052,   309]), tensor([10842, 51699, 10475, 44662]), tensor([10634, 17552, 17761, 11487]), tensor([11959, 75414, 47498, 90064]), tensor([21174, 10178, 10740,   309]), tensor([18745, 19537, 18613, 17940]), tensor([40065, 18213,   309,   118]), tensor([17189, 73996, 30854, 50430]), tensor([17771,   117, 30627, 16366]), tensor([  316, 54625, 39574, 12170]), tensor([35129, 20988, 10300, 10414]), tensor([10242, 57536,   164, 61423]), tensor([10124, 73721, 10167, 49146]), tensor([80307, 79248,   119, 87286]), tensor([96433, 10242, 70121, 10360]), tensor([24745, 43939,   149, 93857]), tensor([  119, 23302,   119, 10185]), tensor([69496, 39662, 70121, 32690]), tensor([57843, 16620, 10124, 10360]), tensor([  107,   119, 36240, 46556]), tensor([41889, 48408, 16486, 20389]), tensor([10185, 59920, 13879, 10414]), tensor([  107,   330, 76271, 61423]), tensor([  315, 56138, 12479, 32258]), tensor([44486, 49460, 23749, 41432]), tensor([46634, 30965, 33605,   113]), tensor([24853, 10353, 25415,   315]), tensor([12415, 12479, 10124, 10506]), tensor([31437, 95592, 40926, 18095]), tensor([10291,   309, 39966,   114]), tensor([92025, 56528,   119,   309]), tensor([91202, 67939,   309, 92315]), tensor([50361, 12313, 79868, 25639]), tensor([  117,   323, 19053, 57247]), tensor([10887, 70463, 20988, 41000]), tensor([10360, 10260, 11827, 10475]), tensor([11943, 12479, 43846,   119]), tensor([11642, 15286, 47110, 11000]), tensor([31623, 33940, 51308, 11041]), tensor([31913, 21174, 10690, 38938]), tensor([  316, 69104,   320, 19796]), tensor([10124, 20751, 10842, 10798]), tensor([41276, 10979, 17830,   309]), tensor([68599, 65393, 24583, 47706]), tensor([  119, 11775,   309, 11637]), tensor([12479,   307, 14313, 13646]), tensor([14907, 35418, 46728, 53464]), tensor([11041, 17632, 22568, 93662]), tensor([38938, 89029, 40509,   117]), tensor([40862, 12230, 68161, 10291]), tensor([  107,   117, 10124, 41492]), tensor([45853, 11041, 46818, 10124]), tensor([38930, 38938, 55973, 39270]), tensor([96493, 40862, 39841, 10584]), tensor([67551,   107, 12402, 10291]), tensor([44249, 45853, 43764, 10776]), tensor([  107, 10260,   316, 67102]), tensor([  119, 42735, 14148, 14434]), tensor([  100, 37354, 92018,   117]), tensor([34399, 10386, 11141,   126]), tensor([10178,   107, 86426, 14310]), tensor([  117,   119, 10396,   117]), tensor([76143,   309, 71740,   309]), tensor([10396, 21266, 18168, 42609]), tensor([10291, 10226,   323, 14380]), tensor([92178, 41592, 19646, 10506]), tensor([10640, 51489, 44052, 17940]), tensor([  117, 12347, 10475,   118]), tensor([10242,   316,   119, 54195]), tensor([39848, 10432, 56162, 10242]), tensor([89901, 11637, 85062, 14608]), tensor([41557, 14664, 27221, 13546]), tensor([30589, 36777, 11182, 87441]), tensor([16352, 11994, 10291, 65131]), tensor([  136, 13970, 11321, 15702]), tensor([  323, 86368, 50208, 41000]), tensor([12682, 10236, 11994, 10475]), tensor([10634, 12415, 39375,   117]), tensor([22060, 19458, 10236, 27082]), tensor([30703, 43553, 42026, 67631]), tensor([  117, 16151, 44464, 15497]), tensor([  316, 10242, 18745, 10360]), tensor([80554, 33039, 45824, 46556]), tensor([10178, 32391, 12690,   309]), tensor([  117, 11569, 11182,   315]), tensor([12331,   107, 10432, 10506]), tensor([12479, 10842, 58865, 18095]), tensor([16696, 40770,   325,   119]), tensor([77230, 11569, 60717, 14971]), tensor([10178,   316, 12954, 10291]), tensor([  119, 72786, 49385, 28598]), tensor([16646, 13192, 11041, 65862]), tensor([37956,   323, 33039, 10260]), tensor([10758, 70463, 17632, 31935]), tensor([  117, 10260, 77230, 87286]), tensor([14321, 10124, 55417, 10260]), tensor([72621, 78836, 45853, 19458]), tensor([11843, 13547, 10260, 56102]), tensor([62920, 11569, 37412, 50914]), tensor([46634,   325, 11090, 90137]), tensor([62703, 56254, 11154, 34797]), tensor([10242, 10236,   118, 18095]), tensor([42815,   107,   315, 37621]), tensor([12415,   119, 52068, 41088]), tensor([  117,   316, 43997, 51582]), tensor([  100, 11484, 33683, 12479]), tensor([50361, 38938, 64486, 12356]), tensor([36863, 11749, 12043, 87222]), tensor([10580,   325,   310,   309]), tensor([14157, 56254, 24529, 54792]), tensor([  119, 13192, 10475,   119]), tensor([10291, 66636,   119, 16747]), tensor([92025,   117, 13271, 10242]), tensor([91202, 17942, 20940, 12331]), tensor([27907,   117, 17667, 11841]), tensor([12286, 19458, 38701, 47706]), tensor([37458, 46909, 20431, 11637]), tensor([10690,   320, 14991, 13646]), tensor([  315, 10291, 62229, 53464]), tensor([10291, 68092, 10328, 10392]), tensor([87686, 19284, 17552, 90137]), tensor([42048, 10124, 37933, 34797]), tensor([10124, 11958, 34216, 18095]), tensor([41276, 13526, 16548, 37621]), tensor([10798, 11007, 10124, 14549]), tensor([65393, 12479, 48195, 57759]), tensor([11261,   325, 12402, 70275]), tensor([26375, 14520, 23302, 11779]), tensor([47618, 13393, 48187, 12063]), tensor([  100, 11827, 11303, 10124]), tensor([87241, 14601,   119, 10360]), tensor([10185, 11405, 12356, 46556]), tensor([71400, 15932, 90137, 10328]), tensor([  315,   119, 85354,   309]), tensor([24853, 19458, 77872, 92315]), tensor([12415, 11779, 10178, 25639]), tensor([46818, 71976, 56329, 57247]), tensor([10353, 19458, 24698, 10124]), tensor([  326, 46198, 10360, 20389]), tensor([74632, 54900, 59409, 54792]), tensor([24139,   330, 80565,   119]), tensor([12109, 56138, 33354,   309]), tensor([  119, 49460, 25634, 20657]), tensor([33878, 30965,   309, 10582]), tensor([10360, 10353, 16696,   117]), tensor([74810, 45718,   119, 10291]), tensor([  117, 11429, 82006, 18296]), tensor([10887, 10242, 53557, 14380]), tensor([  309, 33039, 21991, 10506]), tensor([30928, 10226,   311,   117]), tensor([46919, 94890, 92545, 23337]), tensor([18509, 41592, 12630, 27907]), tensor([11769, 15491, 11090, 10936]), tensor([22889, 81908, 10758, 39794]), tensor([12479, 80538, 15759, 20953]), tensor([11432,   315, 39610, 13147]), tensor([11095, 10124,   325, 41000]), tensor([12234, 37489, 53296, 10475]), tensor([13141, 29365, 22583, 10124]), tensor([18745,   309, 18745, 20389]), tensor([40065,   107, 25511, 54792]), tensor([17189,   314, 92026,   309]), tensor([17771, 18076, 15820, 17940]), tensor([  119, 45905,   309,   118]), tensor([10291, 10468, 11136, 50430]), tensor([92025, 74517, 15428,   325]), tensor([91202, 12253, 11346, 53296]), tensor([78567,   107,   323, 48197]), tensor([  117,   309, 19646, 16765]), tensor([10887, 59920, 44052, 10124]), tensor([10124, 12750, 11346, 11154]), tensor([10291,   119,   320,   110]), tensor([95458, 10291, 11090, 10291]), tensor([  326, 15451, 12945, 69131]), tensor([74632,   307,   119,   323]), tensor([24139, 35418,   320, 59638]), tensor([12109, 17632, 41529, 76631]), tensor([17314, 89029, 10571, 60508]), tensor([87241, 10475, 15711, 86517]), tensor([  107, 19458, 13383, 12230]), tensor([  314, 43553, 12630, 37423]), tensor([15609, 14128, 16421, 13070]), tensor([10376, 10242, 10571, 36146]), tensor([  107, 33039, 74884,   119]), tensor([48984, 32391, 10396,   320]), tensor([16646, 11569, 12448, 19458]), tensor([39841, 17386, 74729, 79699]), tensor([17645, 73863,   119,   309]), tensor([  119, 46790,   309, 59247]), tensor([10124, 10124, 26548, 69800]), tensor([83937, 73175, 10396, 17940]), tensor([13404, 52094, 65375,   118]), tensor([  117, 21133, 11583, 54195]), tensor([80863, 71684, 17965, 27143]), tensor([54001,   117, 12951,   308]), tensor([59980, 14900, 84983, 35026]), tensor([46818, 20374, 85811, 33923]), tensor([10353, 10395, 87241,   322]), tensor([  326,   315, 10475, 47719]), tensor([74632, 28123, 36240, 14278]), tensor([24139, 19284, 76993, 50480]), tensor([10580,   119, 12109, 51317]), tensor([16352, 56528,   315, 34542]), tensor([89901, 39665, 85811, 20389]), tensor([41557, 10396, 11854, 62370]), tensor([94236, 27907, 73386,   100]), tensor([10185, 44044, 10328,   309]), tensor([12313, 11779,   119, 64048]), tensor([  309, 46045, 27919,   100]), tensor([10212, 10291, 25542,   332]), tensor([10582, 21732, 12630,   100]), tensor([  117, 54687, 16421,   119]), tensor([  307, 96469, 10571, 29004]), tensor([10963, 27432, 74884, 14159]), tensor([11041, 10236, 20751, 47309]), tensor([11163,   117, 10979, 14912]), tensor([90516, 13831, 60631,   117]), tensor([83116,   335, 14152, 17940]), tensor([12043, 17390, 17310,   118]), tensor([34399, 44052,   117, 28150]), tensor([10178, 12405, 10887,   102]), tensor([  100, 89912, 36240,     0]), tensor([  309,   117, 34930,     0]), tensor([10520, 10887, 10226,     0]), tensor([10582, 10962, 10396,     0]), tensor([  119, 33403, 10360,     0]), tensor([11000, 69032, 10506,     0]), tensor([27082, 65976, 79648,     0]), tensor([45320, 93662, 11863,     0]), tensor([16820, 13107, 90595,     0]), tensor([10178, 35239, 20485,     0]), tensor([12861,   117, 63165,     0]), tensor([  117,   307, 73721,     0]), tensor([10124, 10396, 11261,     0]), tensor([41276, 12479,   320,     0]), tensor([10798, 69550, 11090,     0]), tensor([65393, 79815, 12945,     0]), tensor([11440, 11440, 10124,     0]), tensor([80863, 38970, 61253,     0]), tensor([54001,   100,   117,     0]), tensor([59980, 12331,   315,     0]), tensor([34399, 86074, 95235,     0]), tensor([10178, 10353, 37621,     0]), tensor([10741, 58050, 29071,     0]), tensor([27907, 10414, 52397,     0]), tensor([33060, 16312, 10690,     0]), tensor([10300, 30968, 10291,     0]), tensor([  309, 10414, 22740,     0]), tensor([29401, 17830, 11749,     0]), tensor([12783, 92112, 14761,     0]), tensor([24853, 11141, 10260,     0]), tensor([12415, 92315,   308,     0]), tensor([  309, 29409, 10260,     0]), tensor([42481, 30761, 11027,     0]), tensor([10520,   119, 95036,     0]), tensor([10369,   309, 57258,     0]), tensor([  119, 33905, 24508,     0]), tensor([27907, 59920, 59409,     0]), tensor([32085, 10236, 10376,     0]), tensor([33878, 50361, 18183,     0]), tensor([10185, 10178, 10124,     0]), tensor([  309,   117, 92457,     0]), tensor([10124, 10887, 10395,     0]), tensor([80307, 51582,   119,     0]), tensor([96433,   320, 10842,     0]), tensor([24745, 12189, 11321,     0]), tensor([90137,   117, 50208,     0]), tensor([15575,   325, 10260,     0]), tensor([38967, 69317, 12479,     0]), tensor([54586, 10634, 79994,     0]), tensor([13448, 34933, 40054,     0]), tensor([19285, 11569, 10414,     0]), tensor([12682, 23029,   308,     0]), tensor([10634, 79380, 10260,     0]), tensor([22060, 12773, 11027,     0]), tensor([30703, 73721, 10842,     0]), tensor([  119, 79248, 53058,     0]), tensor([29317, 10242, 10185,     0]), tensor([28639, 23302, 36240,     0]), tensor([24853, 39662, 16486,     0]), tensor([12415, 16306, 10581,     0]), tensor([10291, 69104,   118,     0]), tensor([37800,   117, 80947,     0]), tensor([55391,   322, 83730,     0]), tensor([63636, 37956, 14961,     0]), tensor([15097, 12539, 18076,     0]), tensor([10584,   309, 10736,     0]), tensor([13461, 53222,   118,     0]), tensor([27982, 26667, 11769,     0]), tensor([50361, 23368, 58671,     0]), tensor([  117, 10226, 10353,     0]), tensor([10887,   119, 10225,     0]), tensor([10535, 11000, 17965,     0]), tensor([14350, 10360, 34918,     0]), tensor([34399, 89692, 11943,     0]), tensor([10178, 10690, 45697,     0]), tensor([  100,   117, 10475,     0]), tensor([94236, 17552,   117,     0]), tensor([12313, 11136, 29995,     0]), tensor([  119, 85905, 14608,     0]), tensor([65521, 11372, 52303,     0]), tensor([10726, 33351, 10328,     0]), tensor([12843, 11484, 10148,     0]), tensor([27715, 22217, 67515,     0]), tensor([12088, 75364,   119,     0]), tensor([  102,   309, 17965,     0]), tensor([    0, 59920, 17265,     0]), tensor([    0, 10236, 10401,     0]), tensor([    0, 17928, 43446,     0]), tensor([    0, 20988,   309,     0]), tensor([    0, 28334, 16696,     0]), tensor([    0, 17632, 59409,     0]), tensor([    0,   117, 53123,     0]), tensor([    0, 13831, 12234,     0]), tensor([    0, 10360, 10396,     0]), tensor([    0, 14597, 57267,     0]), tensor([    0, 42061, 74512,     0]), tensor([    0,   117, 31337,     0]), tensor([    0, 10887,   119,     0]), tensor([    0, 10962, 10291,     0]), tensor([    0, 33403, 46620,     0]), tensor([    0, 17961, 47929,     0]), tensor([    0, 11041, 11141,     0]), tensor([    0, 40734, 19458,     0]), tensor([    0, 63261, 46198,     0]), tensor([    0, 10300, 38903,     0]), tensor([    0,   323, 48178,     0]), tensor([    0,   307, 83288,     0]), tensor([    0, 14052, 96334,     0]), tensor([    0, 64032,   311,     0]), tensor([    0, 95903, 92545,     0]), tensor([    0, 10360, 89072,     0]), tensor([    0, 52930, 10185,     0]), tensor([    0, 12719, 20004,     0]), tensor([    0, 10640, 36421,     0]), tensor([    0,   119, 10436,     0]), tensor([    0, 27082,   117,     0]), tensor([    0, 16820, 21654,     0]), tensor([    0, 10178, 24732,     0]), tensor([    0, 12861, 15428,     0]), tensor([    0, 11041, 60006,     0]), tensor([    0, 38938, 12843,     0]), tensor([    0, 56751, 12920,     0]), tensor([    0,   117, 27880,     0]), tensor([    0, 10887, 18505,     0]), tensor([    0,   309, 47771,     0]), tensor([    0, 74584, 10124,     0]), tensor([    0, 14155, 46818,     0]), tensor([    0, 83628, 55973,     0]), tensor([    0, 59920, 39841,     0]), tensor([    0, 12750, 12402,     0]), tensor([    0, 36884, 43764,     0]), tensor([    0, 17552,   117,     0]), tensor([    0, 14608, 12472,     0]), tensor([    0, 58211, 10291,     0]), tensor([    0, 12843, 33068,     0]), tensor([    0, 85825, 45801,     0]), tensor([    0, 10979, 32550,     0]), tensor([    0, 10124, 14884,     0]), tensor([    0, 82102,   315,     0]), tensor([    0,   315, 10957,     0]), tensor([    0, 26623, 43714,     0]), tensor([    0, 14152, 11827,     0]), tensor([    0,   309, 11854,     0]), tensor([    0, 26036, 58671,     0]), tensor([    0, 47902, 10328,     0]), tensor([    0, 39404, 10225,     0]), tensor([    0,   119, 53222,     0]), tensor([    0, 65521,   315,     0]), tensor([    0, 10726, 44114,     0]), tensor([    0, 12843, 12539,     0]), tensor([    0, 27715,   119,     0]), tensor([    0, 12088, 13831,     0]), tensor([    0,   102, 20634,     0]), tensor([    0,     0, 14961,     0]), tensor([    0,     0, 74691,     0]), tensor([    0,     0, 27919,     0]), tensor([    0,     0, 25542,     0]), tensor([  0,   0, 309,   0]), tensor([    0,     0, 73721,     0]), tensor([    0,     0, 20131,     0]), tensor([    0,     0, 42797,     0]), tensor([  0,   0, 320,   0]), tensor([    0,     0, 13461,     0]), tensor([    0,     0, 44013,     0]), tensor([    0,     0, 11487,     0]), tensor([    0,     0, 19458,     0]), tensor([    0,     0, 55973,     0]), tensor([    0,     0, 63452,     0]), tensor([    0,     0, 11041,     0]), tensor([    0,     0, 23436,     0]), tensor([  0,   0, 311,   0]), tensor([    0,     0, 92545,     0]), tensor([  0,   0, 119,   0]), tensor([  0,   0, 102,   0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0])], 'token_type_ids': [tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 1, 0]), tensor([0, 0, 1, 0]), tensor([0, 0, 1, 0]), tensor([0, 0, 1, 0]), tensor([1, 0, 1, 0]), tensor([1, 0, 1, 0]), tensor([1, 0, 1, 0]), tensor([1, 0, 1, 1]), tensor([1, 0, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 0, 1, 0]), tensor([0, 0, 1, 0]), tensor([0, 0, 1, 0]), tensor([0, 0, 1, 0]), tensor([0, 0, 1, 0]), tensor([0, 0, 1, 0]), tensor([0, 0, 1, 0]), tensor([0, 0, 1, 0]), tensor([0, 0, 1, 0]), tensor([0, 0, 1, 0]), tensor([0, 0, 1, 0]), tensor([0, 0, 1, 0]), tensor([0, 0, 1, 0]), tensor([0, 0, 1, 0]), tensor([0, 0, 1, 0]), tensor([0, 0, 1, 0]), tensor([0, 0, 1, 0]), tensor([0, 0, 1, 0]), tensor([0, 0, 1, 0]), tensor([0, 0, 1, 0]), tensor([0, 0, 1, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0])], 'attention_mask': [tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 1]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([1, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 1, 1, 0]), tensor([0, 0, 1, 0]), tensor([0, 0, 1, 0]), tensor([0, 0, 1, 0]), tensor([0, 0, 1, 0]), tensor([0, 0, 1, 0]), tensor([0, 0, 1, 0]), tensor([0, 0, 1, 0]), tensor([0, 0, 1, 0]), tensor([0, 0, 1, 0]), tensor([0, 0, 1, 0]), tensor([0, 0, 1, 0]), tensor([0, 0, 1, 0]), tensor([0, 0, 1, 0]), tensor([0, 0, 1, 0]), tensor([0, 0, 1, 0]), tensor([0, 0, 1, 0]), tensor([0, 0, 1, 0]), tensor([0, 0, 1, 0]), tensor([0, 0, 1, 0]), tensor([0, 0, 1, 0]), tensor([0, 0, 1, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0])], 'targets': tensor([1, 0, 0, 0])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AdamW, BertConfig, BertForSequenceClassification"
      ],
      "metadata": {
        "id": "a-g6Iiw6zync"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-multilingual-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKDH7Cxl2jvs",
        "outputId": "b33071d3-45ba-4af8-f365-bec2f1c4f84b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )"
      ],
      "metadata": {
        "id": "qAF11sR327as"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 100, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "metadata": {
        "id": "FDlt6GVH280T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score"
      ],
      "metadata": {
        "id": "wd_8WZ4tDHMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_f1(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    print(pred_flat)\n",
        "    labels_flat = labels.flatten()\n",
        "    return f1_score(pred_flat, labels_flat), precision_score(pred_flat, labels_flat), recall_score(pred_flat, labels_flat), accuracy_score(pred_flat, labels_flat)\n",
        "    #return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "metadata": {
        "id": "1RRWD3kB3DSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "metadata": {
        "id": "SJzhKb9a2-3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "loss_values = []\n",
        "\n",
        "for epoch_i in range(0, 4):\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    t0 = time.time()\n",
        "    total_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        b_input_ids = batch['input_ids'].to(device)\n",
        "        b_input_mask = batch['attention_mask'].to(device)\n",
        "        b_labels = batch['targets'].to(device)\n",
        "        b_token_types = batch['token_type_ids'].to(device)\n",
        "        #b_labels = torch.nn.functional.one_hot(b_labels)\n",
        "        #print(b_labels)\n",
        "\n",
        "        model.zero_grad()        \n",
        "\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=b_token_types, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "    eval_f1, eval_precision, eval_recall = 0, 0, 0\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_f1, tmp_eval_precision, tmp_eval_recall, tmp_eval_accuracy = flat_f1(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_f1 += tmp_eval_f1\n",
        "        eval_precision += tmp_eval_precision\n",
        "        eval_recall += tmp_eval_recall\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  f1: {0:.2f}\".format(eval_f1/nb_eval_steps))\n",
        "    print(\"  precision: {0:.2f}\".format(eval_precision/nb_eval_steps))\n",
        "    print(\"  recall: {0:.2f}\".format(eval_recall/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "metadata": {
        "id": "YLL-g9kHvN15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KLAyhUPs0i39"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}